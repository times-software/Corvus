

#########################
    WORKFLOW
#########################
Calculate opcons
1) Calculate optical constants using FEFF.
   input: ['cif_input']
  output: ['opcons']
Required User Input: ['cif_input']
#########################
  END WORKFLOW
#########################



##########################################################
       Component: Li1
##########################################################

Number in unit cell: 2
	K
		XANES
	L1
		XANES


##########################################################
       Component: Li2
##########################################################

Number in unit cell: 2
	K
		XANES
	L1
		XANES


##########################################################
       Component: O3
##########################################################

Number in unit cell: 4
	K
		XANES
	L1
		XANES
	L2
		XANES
	L3
		XANES


			xmu.dat already calculated. Skipping.
			xmu.dat already calculated. Skipping.
			xmu.dat already calculated. Skipping.
			xmu.dat already calculated. Skipping.
			xmu.dat already calculated. Skipping.
0
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/rdinp']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/rdinp
Launching FEFF version FEFF 10.0
RGRID, rgrd;   1.00000E-02
Working in real space.
CIF and ATOMS cards used: ATOMS card will be ignored.
Taking crystal structure from .cif file.
:WARNING  POTENTIALS card contains different number of potentials than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
Now making real-space atoms list from cif input.
nats=            8  lattice=P nshift=            0
using           17 x          17 x           3  unit cells or         6936
atoms
original list:
1    -0.28868     -0.50000     -0.60555
2    -0.28868     -0.50000      0.60555
3     0.00000      0.00000      0.00000
4    -0.57735     -0.00000     -1.21111
5     0.00000      0.00000      0.96721
6     0.28868     -0.50000      0.24389
7     0.28868     -0.50000     -0.24389
8     0.00000      0.00000     -0.96721
Core hole lifetime is   0.002 eV.
Your calculation:
Once upon a time ...
Li L2 edge XANES using FSR corehole.
Using:     * Debye-Waller factors   * Self-Consistent Field potentials
Using cards:   ATOMS CONTROL EXCHANGE RPATH DEBYE PRINT NLEG XANES RGRID EDGE SCF FMS ABSOLUTE REAL TARGET EGRID SETE CIF

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/atomic']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/atomic
Calculating atomic potentials ...

************************************************************************************
************************************************************************************
No electrons in initial state specified by ihole =  3
************************************************************************************
************************************************************************************

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/pot']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/pot
Calculating SCF potentials ...
--------------------------------------------------------------------------
FEFF-MPI using     1 parallel threads.
mpirun has exited due to process rank 0 with PID 0 on

node macchiato exiting improperly. There are three reasons this could occur:
************************************************************************************

************************************************************************************
1. this process did not call "init" before exiting, but others in
Error opening file: apot.bin
the job did. This can cause a job to hang indefinitely while it waits
OPEN returned error number   29
for all processes to call "init". By rule, if one process calls "init",
************************************************************************************
then ALL processes must call "init" prior to termination.
************************************************************************************


2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/screen']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/screen
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ldos']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ldos
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/opconsat']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/opconsat
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/xsph']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/xsph
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 99, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L2/XANES/pot.bin
Image              PC                Routine            Line        Source
xsph               00000000005DF6AB  for__io_return        Unknown  Unknown
xsph               00000000005FE19B  for_open              Unknown  Unknown
xsph               0000000000426BAE  Unknown               Unknown  Unknown
xsph               000000000048064B  Unknown               Unknown  Unknown
xsph               000000000047CE49  Unknown               Unknown  Unknown
xsph               000000000040C4E2  Unknown               Unknown  Unknown
libc-2.28.so       000014F5688F7493  __libc_start_main     Unknown  Unknown
xsph               000000000040C3EE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[14227,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/fms']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/fms
FMS calculation of full Green's function ...
--------------------------------------------------------------------------
FEFF-MPI using     1 parallel threads.
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Error opening file, phase.bin in module rdxsph
with errorcode 0.
Fatal error

CHOPEN
NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/rdinp']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/rdinp
Launching FEFF version FEFF 10.0
RGRID, rgrd;   1.00000E-02
Working in real space.
CIF and ATOMS cards used: ATOMS card will be ignored.
Taking crystal structure from .cif file.
:WARNING  POTENTIALS card contains different number of potentials than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
Now making real-space atoms list from cif input.
nats=            8  lattice=P nshift=            0
using           17 x          17 x           3  unit cells or         6936
atoms
original list:
1    -0.28868     -0.50000     -0.60555
2    -0.28868     -0.50000      0.60555
3     0.00000      0.00000      0.00000
4    -0.57735     -0.00000     -1.21111
5     0.00000      0.00000      0.96721
6     0.28868     -0.50000      0.24389
7     0.28868     -0.50000     -0.24389
8     0.00000      0.00000     -0.96721
Core hole lifetime is   0.002 eV.
Your calculation:
Once upon a time ...
Li L3 edge XANES using FSR corehole.
Using:     * Debye-Waller factors   * Self-Consistent Field potentials
Using cards:   ATOMS CONTROL EXCHANGE RPATH DEBYE PRINT NLEG XANES RGRID EDGE SCF FMS ABSOLUTE REAL TARGET EGRID SETE CIF

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/atomic']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/atomic
Calculating atomic potentials ...
--------------------------------------------------------------------------

mpirun has exited due to process rank 0 with PID 0 on
************************************************************************************
node macchiato exiting improperly. There are three reasons this could occur:
************************************************************************************

No electrons in initial state specified by ihole =  4
1. this process did not call "init" before exiting, but others in
************************************************************************************
the job did. This can cause a job to hang indefinitely while it waits
************************************************************************************
for all processes to call "init". By rule, if one process calls "init",

then ALL processes must call "init" prior to termination.

2. this process called "init", but exited without calling "finalize".
By rule, all processes that call "init" MUST call "finalize" prior to
exiting or it will be considered an "abnormal termination"

3. this process called "MPI_Abort" or "orte_abort" and the mca parameter
orte_create_session_dirs is set to false. In this case, the run-time cannot
detect that the abort call was an abnormal termination. Hence, the only
error message you will receive is this one.

This may have caused other processes in the application to be
terminated by signals sent by mpirun (as reported here).

You can avoid this message by specifying -quiet on the mpirun command line.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/pot']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/pot
Calculating SCF potentials ...
FEFF-MPI using     1 parallel threads.

************************************************************************************
************************************************************************************
Error opening file: apot.bin
OPEN returned error number   29
************************************************************************************
************************************************************************************

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/screen']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/screen
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ldos']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ldos
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/opconsat']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/opconsat
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/xsph']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/xsph
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 99, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L3/XANES/pot.bin
Image              PC                Routine            Line        Source
xsph               00000000005DF6AB  for__io_return        Unknown  Unknown
xsph               00000000005FE19B  for_open              Unknown  Unknown
xsph               0000000000426BAE  Unknown               Unknown  Unknown
xsph               000000000048064B  Unknown               Unknown  Unknown
xsph               000000000047CE49  Unknown               Unknown  Unknown
xsph               000000000040C4E2  Unknown               Unknown  Unknown
libc-2.28.so       0000149D18917493  __libc_start_main     Unknown  Unknown
xsph               000000000040C3EE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[10336,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/fms']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/fms
FMS calculation of full Green's function ...
--------------------------------------------------------------------------
FEFF-MPI using     1 parallel threads.
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Error opening file, phase.bin in module rdxsph
with errorcode 0.
Fatal error

CHOPEN
NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/mkgtr']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/mkgtr
MKGTR: Tracing over Green's function ...
--------------------------------------------------------------------------
Error opening file, phase.bin in module rdxsph
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Fatal error
with errorcode 0.
CHOPEN

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/path']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/path
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 99, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L2/XANES/pot.bin
Image              PC                Routine            Line        Source
path               0000000000438DCB  for__io_return        Unknown  Unknown
path               000000000045042B  for_open              Unknown  Unknown
path               0000000000422EBE  Unknown               Unknown  Unknown
path               0000000000423EDD  Unknown               Unknown  Unknown
path               000000000040A9A2  Unknown               Unknown  Unknown
libc-2.28.so       000014D235803493  __libc_start_main     Unknown  Unknown
path               000000000040A8AE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[10751,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/genfmt']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/genfmt
Calculating EXAFS parameters ...
--------------------------------------------------------------------------
Error opening file, phase.bin in module rdxsph
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Fatal error
with errorcode 0.
CHOPEN

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ff2x']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ff2x
Calculating XAS spectra ...
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 8, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L2/XANES/fort.8
Image              PC                Routine            Line        Source
ff2x               00000000004C3F6B  for__io_return        Unknown  Unknown
ff2x               00000000004F1EAD  for_read_seq_fmt      Unknown  Unknown
ff2x               0000000000474F93  Unknown               Unknown  Unknown
ff2x               0000000000481524  Unknown               Unknown  Unknown
ff2x               00000000004B1EEF  Unknown               Unknown  Unknown
ff2x               00000000004717F7  Unknown               Unknown  Unknown
ff2x               000000000040C8A2  Unknown               Unknown  Unknown
libc-2.28.so       0000147425EF2493  __libc_start_main     Unknown  Unknown
ff2x               000000000040C7AE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[10842,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/sfconv']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/sfconv
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------

--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/mkgtr']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/mkgtr
MKGTR: Tracing over Green's function ...
--------------------------------------------------------------------------
Error opening file, phase.bin in module rdxsph
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Fatal error
with errorcode 0.
CHOPEN

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/path']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/path
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 99, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L3/XANES/pot.bin
Image              PC                Routine            Line        Source
path               0000000000438DCB  for__io_return        Unknown  Unknown
path               000000000045042B  for_open              Unknown  Unknown
path               0000000000422EBE  Unknown               Unknown  Unknown
path               0000000000423EDD  Unknown               Unknown  Unknown
path               000000000040A9A2  Unknown               Unknown  Unknown
libc-2.28.so       00001473B65BD493  __libc_start_main     Unknown  Unknown
path               000000000040A8AE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[10746,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/genfmt']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/genfmt
Calculating EXAFS parameters ...
--------------------------------------------------------------------------
Error opening file, phase.bin in module rdxsph
MPI_ABORT was invoked on rank 0 in communicator MPI_COMM_WORLD
Fatal error
with errorcode 0.
CHOPEN

NOTE: invoking MPI_ABORT causes Open MPI to kill all MPI processes.
You may or may not see output from other processes, depending on
exactly when Open MPI kills them.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ff2x']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ff2x
Calculating XAS spectra ...
forrtl: No such file or directory
forrtl: severe (29): file not found, unit 8, file /home/fdv/src/OpCons/Corvus/corvutils/mp/examples/LiO/Li2O2/mp-841/corvus_oc/Corvus1_FEFF/O3/L3/XANES/fort.8
Image              PC                Routine            Line        Source
ff2x               00000000004C3F6B  for__io_return        Unknown  Unknown
ff2x               00000000004F1EAD  for_read_seq_fmt      Unknown  Unknown
ff2x               0000000000474F93  Unknown               Unknown  Unknown
ff2x               0000000000481524  Unknown               Unknown  Unknown
ff2x               00000000004B1EEF  Unknown               Unknown  Unknown
ff2x               00000000004717F7  Unknown               Unknown  Unknown
ff2x               000000000040C8A2  Unknown               Unknown  Unknown
libc-2.28.so       000015313F160493  __libc_start_main     Unknown  Unknown
ff2x               000000000040C7AE  Unknown               Unknown  Unknown
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

Process name: [[10847,1],0]
Exit code:    29
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/sfconv']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/sfconv
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/rdinp']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/rdinp
Launching FEFF version FEFF 10.0
RGRID, rgrd;   1.00000E-02
Working in real space.
CIF and ATOMS cards used: ATOMS card will be ignored.
Taking crystal structure from .cif file.
:WARNING  POTENTIALS card contains different number of potentials than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
:WARNING   POTENTIALS card contains different atomic number than cif file.  Ignoring POTENTIALS card.
Now making real-space atoms list from cif input.
nats=            8  lattice=P nshift=            0
using           17 x          17 x           3  unit cells or         6936
atoms
original list:
1     0.00000      0.00000     -1.21111
2     0.00000      0.00000      0.00000
3     0.28868      0.50000     -0.60555
4    -0.28868     -0.50000      0.60555
5    -0.57735     -0.00000      0.36166
6    -0.28868      0.50000     -0.36166
7    -0.28868      0.50000     -0.84945
8    -0.57735     -0.00000      0.84945
Core hole lifetime is   0.036 eV.
Your calculation:
Once upon a time ...
Li K edge XANES using FSR corehole.
Using:     * Debye-Waller factors   * Self-Consistent Field potentials
Using cards:   ATOMS CONTROL EXCHANGE RPATH DEBYE PRINT NLEG XANES RGRID EDGE SCF FMS LDOS ABSOLUTE REAL TARGET EGRID SETE CIF

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/atomic']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/atomic
Calculating atomic potentials ...
overlapped atomic potential and density for unique potential    0
overlapped atomic potential and density for unique potential    1
overlapped atomic potential and density for unique potential    2
overlapped atomic potential and density for unique potential    3
overlapped atomic potential and density for unique potential    4
overlapped atomic potential and density for unique potential    5
overlapped atomic potential and density for unique potential    6
overlapped atomic potential and density for unique potential    7
overlapped atomic potential and density for unique potential    8
Done with module: atomic potentials.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/pot']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/pot
Calculating SCF potentials ...
FEFF-MPI using     1 parallel threads.
Muffin tin radii and interstitial parameters [bohr]:
type, norman radius, muffin tin, overlap factor
0  1.40372E+00  1.35365E+00  1.09446E+00
1  1.32373E+00  1.28733E+00  1.07065E+00
2  1.32316E+00  1.28781E+00  1.06843E+00
3  1.21634E+00  1.17117E+00  1.09889E+00
4  1.21634E+00  1.17117E+00  1.09889E+00
5  1.05535E+00  9.99555E-01  1.14975E+00
6  1.05535E+00  9.99555E-01  1.14975E+00
7  1.06163E+00  1.00456E+00  1.15000E+00
8  1.06163E+00  1.00456E+00  1.15000E+00
Core-valence separation energy:  ecv=   -40.000 eV
Initial Fermi level:              mu=    -0.185 eV
Zero temperature single thread
SCF ITERATION NUMBER  1
point #   1  energy = -40.000
FMS for a cluster of   13 atoms around atom type   0
FMS for a cluster of   13 atoms around atom type   1
FMS for a cluster of   13 atoms around atom type   2
FMS for a cluster of   13 atoms around atom type   3
FMS for a cluster of   13 atoms around atom type   4
FMS for a cluster of    8 atoms around atom type   5
FMS for a cluster of    8 atoms around atom type   6
FMS for a cluster of    8 atoms around atom type   7
FMS for a cluster of    8 atoms around atom type   8
point #  20  energy = -28.795
point #  40  energy = -12.698
point #  60  energy = -10.417
point #  80  energy =  -9.695
New Fermi level:    mu=  -9.623 eV  Charge distance=  0.0808 (partial c.d.=  4.4157)
Zero temperature single thread
SCF ITERATION NUMBER  2
point #   1  energy = -40.000
point #  20  energy = -28.862
point #  40  energy =  -9.623
point #  60  energy =  -9.526
New Fermi level:    mu=  -9.249 eV  Charge distance=  0.5787 (partial c.d.=  0.0444)
negative density   0   -0.190 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  3
point #   1  energy = -40.000
point #  20  energy = -28.725
point #  40  energy =  -9.249
point #  60  energy =  -9.158
point #  80  energy =  -7.396
point # 100  energy =  -7.017
New Fermi level:    mu=  -7.017 eV  Charge distance=  0.0393 (partial c.d.=  0.4399)
negative density   0  -10.733 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  4
point #   1  energy = -40.000
point #  20  energy = -29.006
point #  40  energy =  -9.016
point #  60  energy =  -7.030
point #  80  energy =  -7.342
New Fermi level:    mu=  -7.215 eV  Charge distance=  0.1327 (partial c.d.=  0.0578)
negative density   0  -58.702 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  5
point #   1  energy = -40.000
point #  20  energy = -28.730
point #  40  energy =  -8.239
point #  60  energy =  -7.256
point #  80  energy =  -8.387
point # 100  energy =  -7.949
New Fermi level:    mu=  -7.927 eV  Charge distance=  0.0039 (partial c.d.=  0.1728)
negative density   0  -64.727 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  6
point #   1  energy = -40.000
point #  20  energy = -28.975
point #  40  energy =  -8.929
point #  60  energy =  -7.964
New Fermi level:    mu=  -7.959 eV  Charge distance=  0.0022 (partial c.d.=  0.0080)
negative density   0  -69.677 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  7
point #   1  energy = -40.000
point #  20  energy = -28.986
point #  40  energy =  -8.960
New Fermi level:    mu=  -7.961 eV  Charge distance=  0.0041 (partial c.d.=  0.0042)
negative density   0  -78.083 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  8
point #   1  energy = -40.000
point #  20  energy = -28.987
point #  40  energy =  -8.963
New Fermi level:    mu=  -7.965 eV  Charge distance=  0.0018 (partial c.d.=  0.0046)
negative density   0  -81.630 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER  9
point #   1  energy = -40.000
point #  20  energy = -28.988
point #  40  energy =  -8.966
New Fermi level:    mu=  -7.966 eV  Charge distance=  0.0072 (partial c.d.=  0.0004)
negative density   0  -96.740 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER 10
point #   1  energy = -40.000
point #  20  energy = -28.988
point #  40  energy =  -8.967
New Fermi level:    mu=  -7.972 eV  Charge distance=  0.0001 (partial c.d.=  0.0017)
negative density   0  -97.377 - usually harmless precision error, but check DOS if it persists
Zero temperature single thread
SCF ITERATION NUMBER 11
point #   1  energy = -40.000
point #  20  energy = -28.990
point #  40  energy =  -8.973
New Fermi level:    mu=  -7.972 eV  Charge distance=  0.0007 (partial c.d.=  0.0000)
Electronic configuration
type     l     N_el
0     0    0.473
0     1    1.029
0     2    0.000
0     3    0.000
1     0    0.238
1     1    0.508
1     2    0.000
1     3    0.000
2     0    0.235
2     1    0.503
2     2    0.000
2     3    0.000
3     0    0.220
3     1    0.511
3     2    0.000
3     3    0.000
4     0    0.220
4     1    0.511
4     2    0.000
4     3    0.000
5     0    1.888
5     1    4.237
5     2    0.121
5     3    0.000
6     0    1.888
6     1    4.237
6     2    0.121
6     3    0.000
7     0    1.879
7     1    4.283
7     2    0.122
7     3    0.000
8     0    1.879
8     1    4.283
8     2    0.122
8     3    0.000
Charge transfer:  type  charge
0   -0.498
1   -0.254
2   -0.261
3   -0.269
4   -0.269
5    0.245
6    0.245
7    0.284
8    0.284
Convergence reached in   11 iterations.
total time         24.9306s          (communication time     0.0000E+00s)
Done with module: potentials.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/screen']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/screen
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ldos']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ldos
Calculating LDOS ...
FEFF-MPI using     1 parallel threads.
Using   160 energy points
FMS for a cluster of  33 atoms around atom type  0
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  1
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  2
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  27 atoms around atom type  3
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  27 atoms around atom type  4
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  5
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  6
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  7
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
FMS for a cluster of  33 atoms around atom type  8
Energy point    1/ 160
Energy point   10/ 160
Energy point   30/ 160
Energy point   50/ 160
Energy point   70/ 160
Energy point   90/ 160
Energy point  110/ 160
Energy point  130/ 160
Energy point  150/ 160
Writing DOS for atom type     0
Writing DOS for atom type     1
Writing DOS for atom type     2
Writing DOS for atom type     3
Writing DOS for atom type     4
Writing DOS for atom type     5
Writing DOS for atom type     6
Writing DOS for atom type     7
Writing DOS for atom type     8
Done with LDOS.
total time         37.6308s          (communication time     0.0000E+00s)
Done with module: LDOS.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/opconsat']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/opconsat
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/xsph']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/xsph
Calculating cross-section and phases ...
Fixing edge energy from Elam table:
emu =     54.700 eV
absorption cross section
phase shifts for unique potential    0
phase shifts for unique potential    1
phase shifts for unique potential    2
phase shifts for unique potential    3
phase shifts for unique potential    4
phase shifts for unique potential    5
phase shifts for unique potential    6
phase shifts for unique potential    7
phase shifts for unique potential    8
Done with module: cross-section and phases (XSPH).

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/fms']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/fms
FMS calculation of full Green's function ...
FEFF-MPI using     1 parallel threads.
Using   261 energy points.
Applying Debye-Waller factors using a Correlated Debye model.
xprep done
FMS for a cluster of   33 atoms
Energy point    1/ 261
Energy point   10/ 261
Energy point   20/ 261
Energy point   30/ 261
Energy point   40/ 261
Energy point   50/ 261
Energy point   60/ 261
Energy point   70/ 261
Energy point   80/ 261
Energy point   90/ 261
Energy point  100/ 261
Energy point  110/ 261
Energy point  120/ 261
Energy point  130/ 261
Energy point  140/ 261
Energy point  150/ 261
Energy point  160/ 261
Energy point  170/ 261
Energy point  180/ 261
Energy point  190/ 261
Energy point  200/ 261
Energy point  210/ 261
Energy point  220/ 261
Energy point  230/ 261
Energy point  240/ 261
Energy point  250/ 261
Energy point  260/ 261
total time          7.8341s          (communication time     0.0000E+00s)
Done with module: FMS.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/mkgtr']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/mkgtr
MKGTR: Tracing over Green's function ...
Done with module: MKGTR.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/path']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/path
Pathfinder: finding scattering paths...
Preparing plane wave scattering amplitudes
Searching for paths
Rmax  0.1000  keep and heap limits   0.0000000   0.0000000
Preparing neighbor table
Paths found        0   (maxheap, maxscatt       1   0)
Eliminating path degeneracies
0 paths retained.
Done with module: pathfinder.

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/genfmt']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/genfmt
Calculating EXAFS parameters ...
Done with module: EXAFS parameters (GENFMT).

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/ff2x']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/ff2x
Calculating XAS spectra ...
electronic_temperature =    0.00000 (eV)
Done with module: XAS spectra (FF2X: DW + final sum over paths).

--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/sfconv']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/sfconv
--- FDV --- ['-np', '1', '-v', '--bind-to', 'none', '/home/fdv/src/OpCons/feff10/bin/MPI/feff_timer']
			Running executable: mpirun -np 1 -v --bind-to none /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
--------------------------------------------------------------------------
mpirun was unable to launch the specified application as it could not access
or execute an executable:

Executable: /home/fdv/src/OpCons/feff10/bin/MPI/feff_timer
Node: macchiato

while attempting to start process rank 0.
--------------------------------------------------------------------------
